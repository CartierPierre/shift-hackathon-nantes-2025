from typing import TypedDict, Sequence, List, Optional, Dict, Any, Annotated
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain.tools import BaseTool
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import add_messages
from langgraph.prebuilt import ToolNode
from app.infrastructure.llm_client import LLMClient
from app.core.tools import (
    simplify_expr,
    solve_equation,
    derivative,
    integrate_expr,
    factor_expr,
    expand_expr
)
from app.core.tool_utils import tools_condition, create_tool_node_with_fallback
from app.core.requests_questions import fetch_questions
import logging
import uuid
import random

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# D√©finition des types pour notre √©tat
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    current_step: str
    thread_id: str
    questions: List[str]
    analyses: List[Dict[str, Any]]
    waiting_for_answer: bool
    needs_evaluation: bool
    current_question_index: int
    nb_good_answers: int = 0
    category: str = "Conjugaison des verbes du 2√®me groupe"
    course: str = "Fran√ßais"

# Configuration par d√©faut
QUESTIONS_SYSTEM_PROMPT = """
Pose-moi la question suivante : 'Comment conjugue-t-on le verbe boire au pr√©sent ?'
Analyse ma r√©ponse et attribue une note de 1 √† 5.

5 = Tout est correct
4 = Une petite erreur
3 = Quelques erreurs
2 = Beaucoup d'erreurs
1 = Presque tout est faux

Ajoute un commentaire dans un objet JSON avec :

 - note : (le score sur 5)
 - commentaire : un message qui souligne ce que j'ai bien fait et corrige mes erreurs si n√©cessaire

Tu es un adolescent de 12 ans en classe de 6√®me. Ton langage est naturel, simple et un peu familier, comme si tu parlais √† un copain en classe.
"""

DEFAULT_SYSTEM_PROMPT = "Tu es une personne chaleureuse et attentionn√©e. Tu m'aide √† r√©viser ses le√ßons."

from pydantic import BaseModel, Field

class EvaluationResponse(BaseModel):
    note: int = Field(..., ge=1, le=5)
    comment: str

class CompleteEvaluationResponse(BaseModel):
    note: int = Field(..., ge=1, le=5, description="Note sur 5 de la r√©ponse")
    feedback: str = Field(description="Commentaire d√©taill√© sur la r√©ponse")

class AgentResponse(BaseModel):
    content: str
    evaluation: Optional[EvaluationResponse] = None

def create_agent(
    api_key: str,
    system_prompt: Optional[str] = DEFAULT_SYSTEM_PROMPT,
    tools: Optional[List[Dict[str, Any]]] = None,
    thread_id: Optional[str] = None,
    category: str = "",
    course: str = "",
    nb_good_answers: int = 0
):
    try:
        # Initialisation du client LLM
        llm_client = LLMClient(api_key=api_key)
        llm = llm_client.get_llm()

        # Initialisation des outils
        tool_objects = []

        # Ajout des outils math√©matiques par d√©faut
        default_tools = [
            simplify_expr,
            solve_equation,
            derivative,
            integrate_expr,
            factor_expr,
            expand_expr
        ]

        # Ajout des outils par d√©faut
        tool_objects.extend(default_tools)
        logger.info(f"Outils par d√©faut ajout√©s: {[tool.name for tool in default_tools]}")

        # Ajout des outils personnalis√©s
        if tools:
            for tool in tools:
                try:
                    if not all(k in tool for k in ["name", "description", "func"]):
                        logger.error(f"L'outil personnalis√© doit avoir un nom, une description et une fonction: {tool}")
                        continue

                    @tool
                    def custom_tool(**kwargs):
                        return tool["func"](**kwargs)

                    custom_tool.name = tool["name"]
                    custom_tool.description = tool["description"]
                    tool_objects.append(custom_tool)
                    logger.info(f"Outil personnalis√© ajout√©: {tool['name']}")
                except Exception as e:
                    logger.error(f"Erreur lors de la cr√©ation de l'outil personnalis√© {tool.get('name', 'unknown')}: {str(e)}")
                    continue

        if not tool_objects:
            logger.warning("Aucun outil valide n'a √©t√© cr√©√©")

        # Bind des outils au LLM
        llm_with_tools = llm.bind_tools(tool_objects)
        logger.info(f"Outils li√©s au LLM: {[tool.name for tool in tool_objects]}")


        # Fonction pour d√©finir les questions
        def define_questions(state: AgentState) -> AgentState:
            logger.info("D√©finition des questions")

            # Initialisation de l'√©tat si c'est la premi√®re fois
            if not state.get("questions"):
                logger.info("üîÑ Initialisation de l'√©tat")
                state.update({
                    "messages": [HumanMessage(content=state["messages"][0].content)],
                    "questions": [],
                    "analyses": [],
                    "category": state.get("category", "Conjugaison des verbes du 2√®me groupe"),
                    "course": state.get("course", "Fran√ßais"),
                    "waiting_for_answer": False,
                    "needs_evaluation": False,
                    "current_question_index": 0,
                    "nb_good_answers": 0
                })

            # R√©cup√©ration des questions
            questions = fetch_questions(state["category"], state["course"])
            state["questions"] = questions
            logger.info(f"‚ùì Questions d√©finies: {questions}")
            logger.info(f"‚ùî √âtat actuel: {state}")
            return state

        # Fonction pour poser les questions
        def ask_questions(state: AgentState) -> AgentState:
            logger.info("Pose des questions")
            can_ask_question = not state["waiting_for_answer"]
            has_remaining_questions = len(state["questions"]) > state["current_question_index"]

            logger.info(f"üìä Nombre total de questions: {len(state['questions'])}")
            logger.info(f"üìç Index question actuelle: {state['current_question_index']}")
            logger.info(f"üí£ Attente de r√©ponse ?: {state['waiting_for_answer']}")

            # Si on attend d√©j√† une r√©ponse, on ne fait rien
            if state["waiting_for_answer"]:
                logger.info("En attente de r√©ponse, pas d'action n√©cessaire")
                return state

            # Si on a encore des questions √† poser
            if has_remaining_questions:
                next_question = state["questions"][state["current_question_index"]]
                state["messages"].append(AIMessage(content=next_question))
                state["waiting_for_answer"] = True
                state["needs_evaluation"] = True
                state["current_question_index"] += 1
                logger.info(f"Question pos√©e: {next_question}")
            else:
                logger.info("Toutes les questions ont √©t√© pos√©es")
                state["current_step"] = END

            return state

        # Fonction pour √©valuer les r√©ponses
        def evaluate_answers(state: AgentState) -> AgentState:
            logger.info("üö© √âvaluation des r√©ponses")

            # V√©rification des indicateurs d'√©tat
            if not state["needs_evaluation"]:
                logger.warning("Pas besoin d'√©valuation selon l'√©tat")
                return state

            # R√©cup√©ration des messages
            messages = state["messages"]

            # V√©rification qu'il y a au moins 2 messages (question et r√©ponse)
            if len(messages) < 2:
                logger.warning("Pas assez de messages pour √©valuer")
                return state

            # V√©rification que le dernier message est humain (r√©ponse de l'utilisateur)
            if not isinstance(messages[-1], HumanMessage):
                logger.warning("Le dernier message n'est pas un message humain")
                return state

            # V√©rification que l'avant-dernier message est IA (question)
            if not isinstance(messages[-2], AIMessage):
                logger.warning("L'avant-dernier message n'est pas un message IA")
                return state

            # R√©cup√©ration question/r√©ponse
            last_question = messages[-2].content
            last_answer = messages[-1].content
            logger.info(f"üîç Question: {last_question}")
            logger.info(f"üîç R√©ponse: {last_answer}")

            if not last_question or not last_answer:
                logger.warning("Impossible de trouver la question et la r√©ponse")
                return state

            # Cr√©ation du prompt pour l'√©valuation
            evaluation_prompt =f"""
            Role : You are a referent student in 6-grade.
            Contexte : You are studying with a friend and classmate.
                        Question :  {last_question}
                        Anwser : {last_answer}
            Task : Your evaluate the answer of your classmate basing into these 2 criterias:
                    1. Quality of the answer : excellent, very good, good, average, to be improved. You don't give a note but a quality appreciation such as "your answer is correct but incomplete"
                    2. A detailled feedback. You highlight the positive points of the answer and you correct all the mistakes.
                    3. A cheering idioms or expression such as "you'll make it", "bravo".
            Tone : You talk to a good friend who is also in your 6-grade class.
                    There are compassion and kindness.
                    You are passionnate and you have mentorship abilities to lead the classmate to improve himself.
            Format : You answer in French. You speak in a familiar way as a teenager.
            """
            # Configuration du LLM avec sortie structur√©e
            structured_llm = llm.with_structured_output(CompleteEvaluationResponse)

            try:
                # √âvaluation de la r√©ponse
                evaluation = structured_llm.invoke(evaluation_prompt)

                # Ajout de l'√©valuation √† la liste des analyses
                state["analyses"].append({
                    "question": last_question,
                    "answer": last_answer,
                    "evaluation": evaluation.dict()
                })

                # choix du message d'encouragement √† afficher
                msg = f""

                if evaluation.note >=2:
                    state["nb_good_answers"] += 1
                else:
                    state["nb_good_answers"] = 0

                if state["nb_good_answers"] >= 2:
                    # message de f√©licitations si au moins 2 bonnes r√©ponses de suite
                    msgs = [f"Bravo ! Cela fait {state["nb_good_answers"]} bonnes r√©ponses de suite !",
                        f"Tu peux √™tre fier de toi ! C'est une s√©rie gagnante de {state["nb_good_answers"]} bonnes r√©ponses d'affil√©e !",
                        f"Bravo ! Tu es un crack ! {state["nb_good_answers"]} bonnes r√©ponses de suite !"
                    ]
                    choix=random.choice(range(0, 2))
                    msg=msgs[choix]

                # Ajout du feedback dans les messages
                feedback_message = f"""
                {evaluation.feedback}

                {msg}
                """

                state["messages"].append(AIMessage(content=feedback_message))
                logger.info(f"√âvaluation effectu√©e : {evaluation.dict()}")

                # Mise √† jour des indicateurs apr√®s √©valuation
                state["waiting_for_answer"] = False
                state["needs_evaluation"] = False

            except Exception as e:
                logger.error(f"Erreur lors de l'√©valuation : {str(e)}")
                error_message = "D√©sol√©, j'ai eu un probl√®me pour √©valuer ta r√©ponse. On peut continuer avec la question suivante !"
                state["messages"].append(AIMessage(content=error_message))
                # En cas d'erreur, on r√©initialise aussi les indicateurs

            logger.info(f"‚ùî √âtat actuel: {state}")
            return state

        # Fonction pour d√©terminer la prochaine √©tape
        def determine_next_step(state: AgentState) -> str:
            logger.info("üîÑ D√©termination de la prochaine √©tape")
            logger.info(f"üìä √âtat actuel: waiting_for_answer={state['waiting_for_answer']}, needs_evaluation={state['needs_evaluation']}")
            logger.info(f"üìù Nombre d'analyses: {len(state['analyses'])}, Nombre de questions: {len(state['questions'])}")

            # Si nous attendons une r√©ponse
            if state["waiting_for_answer"]:
                logger.info("‚è≥ En attente d'une r√©ponse de l'utilisateur")
                return "evaluate_answers"

            # Si nous avons besoin d'√©valuer une r√©ponse
            if state["needs_evaluation"]:
                logger.info("üìù Besoin d'√©valuer la derni√®re r√©ponse")
                return "evaluate_answers"

            # Si nous avons des questions restantes
            if state["current_question_index"] < len(state["questions"]):
                logger.info("‚ùì Questions restantes √† traiter")
                return "ask_questions"

            # Si toutes les questions ont √©t√© trait√©es
            logger.info("‚úÖ Toutes les questions ont √©t√© trait√©es")
            return END

        # Cr√©ation du graphe
        workflow = StateGraph(AgentState)

        # Ajout des n≈ìuds
        workflow.add_node("define_questions", define_questions)
        workflow.add_node("ask_questions", ask_questions)
        workflow.add_node("evaluate_answers", evaluate_answers)

        # Cr√©ation du n≈ìud d'outils avec gestion des erreurs
        tool_node = create_tool_node_with_fallback(tool_objects)
        workflow.add_node("execute_tools", tool_node)

        # Configuration du flux
        workflow.add_edge(START, "define_questions")

        # Edge conditionnel apr√®s define_questions
        workflow.add_conditional_edges(
            "define_questions",
            determine_next_step,
            {
                "evaluate_answers": "evaluate_answers",
                "ask_questions": "ask_questions",
                END: END
            }
        )

        workflow.add_edge("evaluate_answers", "ask_questions")
        workflow.add_edge("ask_questions", END)

        # Initialisation de la m√©moire
        memory = MemorySaver()
        logger.info(f"Agent cr√©√© avec m√©moire pour le thread: {thread_id}")

        # Compilation du graphe avec la m√©moire
        chain = workflow.compile(checkpointer=memory)
        logger.info("Agent cr√©√© avec succ√®s")

        # Fonction d'entr√©e pour le graphe
        def run_graph(user_message: str, thread_id: str) -> Dict[str, Any]:
            """
            Fonction d'entr√©e pour ex√©cuter le graphe avec un message utilisateur.

            Args:
                user_message: Le message de l'utilisateur
                thread_id: L'identifiant unique de la conversation

            Returns:
                Dict[str, Any]: L'√©tat final apr√®s ex√©cution du graphe
            """
            try:
                # Configuration du graphe avec le thread_id
                config = {
                    "configurable": {
                        "thread_id": thread_id,
                        "system_prompt": system_prompt
                    }
                }

                # Cr√©ation de l'√©tat initial minimal
                state = {
                    "messages": [HumanMessage(content=user_message)],
                    "thread_id": thread_id
                }

                logger.info(f"D√©marrage du graphe avec l'√©tat initial: {state}")
                logger.info(f"Configuration du graphe: {config}")

                # Ex√©cution du graphe avec la configuration
                final_state = chain.invoke(state, config=config)
                logger.info("Graphe ex√©cut√© avec succ√®s")
                return final_state
            except Exception as e:
                logger.error(f"Erreur lors de l'ex√©cution du graphe: {str(e)}")
                raise

        return run_graph
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation de l'agent: {str(e)}")
        raise
